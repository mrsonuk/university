{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["q491HYdbIq8T","jkdo7SE6Mw3N","y3OJYgdqN38Y","Zfz0fzTpA-3j"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"ZiWHjXgvEChM","colab_type":"text"},"cell_type":"markdown","source":["# IDS Assignment 2\n","Document your results as well as the way you obtained them in this jupyter notebook. A seperate report (pdf, word etc.) is _not_ required. However, it is necessary that you provide the python code leading to your results as well as textual answers to the assignment questions in this notebook. \n","\n","Do not change the general structure of this notebook, but you can add further markdown or code cells to explain your solutions if necessary. In the end, submit this file in moodle."]},{"metadata":{"id":"3pftdbPtBam4","colab_type":"text"},"cell_type":"markdown","source":["# Preprocessing and Data Quality \n"]},{"metadata":{"id":"fLUecg7SFaoG","colab_type":"text"},"cell_type":"markdown","source":["###Question 1 (Order cancellations)\n","Invoices with a InvoiceNo starting with the letter ‘c’ are order cancellations. Would you recommend keeping the order cancellation in your data set? Also provide a reason for your recommandation. "]},{"metadata":{"id":"xkSmAfDFFycc","colab_type":"text"},"cell_type":"markdown","source":["Your answer:"]},{"metadata":{"id":"5RokWgJyF10J","colab_type":"code","colab":{}},"cell_type":"code","source":["#Modify the data set according to your recommendation"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OugGYUj3HCcD","colab_type":"text"},"cell_type":"markdown","source":["###Question 2 (Empty values)\n","The attributes Description and CustomerID contain empty values. The Country attribute contains an “unspecified” value. For each of the three attributes reason how you would handle these values and why. "]},{"metadata":{"id":"4VQ4ArG8IYhE","colab_type":"text"},"cell_type":"markdown","source":["Your answer:"]},{"metadata":{"id":"k8c855xlILgG","colab_type":"code","colab":{}},"cell_type":"code","source":["#Modify the data set according to your recommendation"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q491HYdbIq8T","colab_type":"text"},"cell_type":"markdown","source":["###Question 3 (Outliers/Noise)\n","Explore into the attributes Quantity and UnitPrice by plotting each attribute visually. Do these attributes contain noise and/or outliers? If so, reason how you would handle them and modify your data set accordingly.\n"]},{"metadata":{"id":"-bOdS-GsJPn3","colab_type":"code","colab":{}},"cell_type":"code","source":["#Your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GEMVC1cfJUec","colab_type":"text"},"cell_type":"markdown","source":["Your explanation:"]},{"metadata":{"id":"jxkDUfn3Lehi","colab_type":"text"},"cell_type":"markdown","source":["#Visualization"]},{"metadata":{"id":"eTMS8jZIL2s_","colab_type":"text"},"cell_type":"markdown","source":["###Question 4 (Stream graph)\n","Create a stream graph that visualizes the number  of purchases (invoices) per country over time.\n","\n","4. (a) Modify the data set to only contain purchases made in the countries Belgium, Ireland (EIRE), France, Germany, the Netherlands, Norway, Portugal, Spain and Switzerland."]},{"metadata":{"id":"Ij3QO-Y_MR3A","colab_type":"code","colab":{}},"cell_type":"code","source":["#your modification"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jQTj0TziMUBu","colab_type":"text"},"cell_type":"markdown","source":["4. (b) Modify the data set such that it shows per month for each country how many purchases were made (i.e. how many invoices were created)."]},{"metadata":{"id":"ADMbVAjOMY-7","colab_type":"code","colab":{}},"cell_type":"code","source":["#your modification"],"execution_count":0,"outputs":[]},{"metadata":{"id":"odTJGKrRMcwG","colab_type":"text"},"cell_type":"markdown","source":["4. (c) Use the modified data to create a stream graph. "]},{"metadata":{"id":"V34Z_doDMg7W","colab_type":"code","colab":{}},"cell_type":"code","source":["#your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XXdSd-59Miy8","colab_type":"text"},"cell_type":"markdown","source":["4. (d) Use this graph to compare the purchases made by each country. "]},{"metadata":{"id":"SutofybpMrKN","colab_type":"text"},"cell_type":"markdown","source":["Your answer:"]},{"metadata":{"id":"jkdo7SE6Mw3N","colab_type":"text"},"cell_type":"markdown","source":["### Question 5 (Heat map)\n","Create a heat map that visualizes how much (in sterling) each country purchases per month. \n","\n","5. (a) Modify the data set to only contain purchases made in the countries Belgium, Ireland (EIRE), France, Germany, the Netherlands, Norway, Portugal, Spain and Switzerland. (Or use the version of the data set that you created for question 4 a)."]},{"metadata":{"id":"5yVXnl6WNZ3N","colab_type":"code","colab":{}},"cell_type":"code","source":["#your modification"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cEV9doRYNanl","colab_type":"text"},"cell_type":"markdown","source":["5. (b) Modify the data set such that it shows per month how much money (in sterling) was spent in the shop per country."]},{"metadata":{"id":"WdKt5vIGNfAF","colab_type":"code","colab":{}},"cell_type":"code","source":["#your modification"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z9OxLF1xNg6d","colab_type":"text"},"cell_type":"markdown","source":["5. (c) Use the modified data to create a heat map. "]},{"metadata":{"id":"BeqD_H9BNsWd","colab_type":"code","colab":{}},"cell_type":"code","source":["#your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K_DdfvB7NuH1","colab_type":"text"},"cell_type":"markdown","source":["5. (d) Compare the amount of the purchases over time and between each country. "]},{"metadata":{"id":"nYBMrbELNz9e","colab_type":"text"},"cell_type":"markdown","source":["Your answer:"]},{"metadata":{"id":"y3OJYgdqN38Y","colab_type":"text"},"cell_type":"markdown","source":["###Question 6 (Interpretation)\n","Compare the results obtained from the stream graph and the heat map. Is there a relation between the number of purchases and the amount purchased in sterling?\n"]},{"metadata":{"id":"WSReYGIFN_c9","colab_type":"text"},"cell_type":"markdown","source":["Your answer:"]},{"metadata":{"id":"Zfz0fzTpA-3j","colab_type":"text"},"cell_type":"markdown","source":["# Clustering\n","Presume that the business analyst would like to cluster transactions with similar types of products into the same group (here don’t consider the quantity of the products). For each product, only use its ‘StockCode’ to represent it. All the results here should be based on the preprocessed data set obtained from question 1 to 3 of this assignment. Presume that this obtained data set from question 1 to 3 has a variable name ‘cluster_dataset’ and is expressed by Pandas DataFrame in your code.\n","\n","###Question 7 (Data transformation and clustering)\n","7. (a) Calculate and show the number of occurrences of each product in data set   ‘cluster_dataset’. For example, if a product appears in a transaction, then its occurrence number will be increased by 1 (do not consider the quantity of this product here). Preserve the 100 most frequent products and remove all the other products in ‘cluster_dataset’. For example, if a row in ‘cluster_dataset’ contains unqualified product, then remove this row from ‘cluster_dataset’. Show the new ‘cluster_dataset’ in your result.\n"]},{"metadata":{"id":"tV-Zzxo-BJwS","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zvM5UjQclH0j","colab_type":"text"},"cell_type":"markdown","source":["7. (b) Based on question a, please reorganize the data from ‘cluster_dataset’ and generate a new data set ‘cluster_dataset_new’ which has a suitable format (for k-means) for solving the transaction clustering problem mentioned above. Show the data from ‘cluster_dataset_new’ by using Pandas DataFrame in your result, where the index should be consistent with the values of 'InvoiceNo', the column name should be consistent with the values of 'StockCode' and each element in this DataFrame should have a value 0 or 1."]},{"metadata":{"id":"VMLEa0gslXcD","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ox2r6Nvxlcm4","colab_type":"text"},"cell_type":"markdown","source":["7. (c) Try values 2, 3, 4, 5 for parameter 'n_clusters' for the k-means function from Scikit-Learn over the data set ‘cluster_dataset_new’ generated in question b. Show the ‘within cluster variation’ (also called ‘sum of squared distances’) of the generated clusters for each different setting for ‘n_clusters’ in your result. Also write down the value that you have tried for setting 'n_clusters' which can help generate the best clustering results and explain how you make this decision."]},{"metadata":{"id":"Q68MWZDIlrOn","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q_X_6ev3Ojqu","colab_type":"text"},"cell_type":"markdown","source":["Your explanation:"]},{"metadata":{"id":"ekfbWzxdBKU8","colab_type":"text"},"cell_type":"markdown","source":["# Frequent Itemsets and Association Rules\n"," For the clusters output by k-means function with the best 'n_clusters' from question 7, the business analyst now would like to research on the frequent purchase behaviours and specific purchase rules for each cluster.\n","###Question 8 (Data transformation and mining frequent itemsets and association rules)\n","8. (a) Set the minimum support for finding the frequent purchase behaviours to 0.2. Please provide the business analyst with the qualified purchase behaviours. For each product, only use its ‘StockCode’ to represent it. Also show the data set prepared for each cluster for mining the frequent behaviours by using Pandas DataFrame in your result, the data set for the cluster k should have the variable name 'fpb_data_k' in your code."]},{"metadata":{"id":"kZPTJqxmBOr3","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"908qPmK3Oyzf","colab_type":"text"},"cell_type":"markdown","source":["8. (b) Furthermore, the business analyst would like to analyze the purchase behaviour of the citizens from ‘United Kingdom’ for each cluster. Specifically speaking, he wants to discover if there exist some rules which indicate that the citizens from ‘United Kingdom’ tend to buy some specific products for each cluster. Set the minimum support to 0.2, minimum confidence to 0.7. Please discover and show such rules (only show the rules with ‘United Kingdom’ appearing in antecedents in the rules) for each cluster for the business analyst. Also show the data sets prepared for each cluster for mining the relevant rules by using Pandas DataFrame in you result, the data set for cluster k should have the variable name 'r_data_k' in your code."]},{"metadata":{"id":"246Z21sPnlRV","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G9-Gqj3qBXjw","colab_type":"text"},"cell_type":"markdown","source":["#Text Mining\n","###Question 12 (Model based on binary document-term matrix)\n","Perform preprocessing on the corpus (all lowercase, no punctuation, tokenization, stemming, stopword removal) and obtain a binary document-term matrix; train a logistic classifier."]},{"metadata":{"id":"t1JdbpAJBTHQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# nltk's default stoplist:\n","from nltk.corpus import stopwords\n","stoplist = set(stopwords.words('english'))\n","\n","# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HFoouarHPWfK","colab_type":"text"},"cell_type":"markdown","source":["###Question 13 (Model based on document-term matrix of counts)\n","Perform preprocessing on the corpus (all lowercase, no punctuation, tokenization, stemming, stopword removal) and obtain a document-term matrix of counts; train a logistic classifier.\n"]},{"metadata":{"id":"vDjnCPi2BaNm","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SpcgbGTOPp3A","colab_type":"text"},"cell_type":"markdown","source":["###Question 14 (Model based on tf-idf document-term matrix)\n","Perform preprocessing on the corpus (all lowercase, no punctuation, tokenization, stemming, stopword removal) and obtain a tf-idf scores document-term matrix; train a logistic classifier.\n"]},{"metadata":{"id":"1FJgzMKlPus6","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4IV9XRbKPwGW","colab_type":"text"},"cell_type":"markdown","source":["###Question 15 (Model based on doc2vec)\n","Perform preprocessing on the corpus (all lowercase, no punctuation, tokenization, stemming, stopword removal) and obtain a doc2vec embedding in order to reduce the dimension of the document vector to 300; use the doc2vec model you just trained to convert the training set to a set of document vectors; train a logistic classifier.\n"]},{"metadata":{"id":"cv8oGoxlQCzm","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nUTK6JL6QKxX","colab_type":"text"},"cell_type":"markdown","source":["###Question 16 (Evaluation)\n","16. (a) Predict the classification with the four models on the test data.\n"]},{"metadata":{"id":"HoGSkKTDQSLP","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RiB6Dm6KQVY3","colab_type":"text"},"cell_type":"markdown","source":["16. (b) Obtain confusion matrices for the four different models.\n"]},{"metadata":{"id":"Vvp_5hEWQZN-","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gi9Xb-JoQagB","colab_type":"text"},"cell_type":"markdown","source":["16. (c) Obtain accuracy and f1 score for the four different models.\n"]},{"metadata":{"id":"sem1ajYSQfI_","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9-sy3DloQg72","colab_type":"text"},"cell_type":"markdown","source":["16. (d) Briefly comment on the quality of the predictions for the four models."]},{"metadata":{"id":"42-LzxBoQn-x","colab_type":"text"},"cell_type":"markdown","source":["_Your comment:_\n"]},{"metadata":{"id":"pGImEFS3BPcJ","colab_type":"text"},"cell_type":"markdown","source":["# Process Mining\n","For this part, refer to the online docs of pm4py. You will find particularly of interest the documentation on filtering (https://pm4py.github.io/filtering.html, or on the new website http://pm4py.pads.rwth-aachen.de/documentation/filtering-logs/). \n","important: if you did not do it in the instruction, you should make sure to have the latest pm4py version: to get it is sufficient to type `pip install pm4py --upgrade` from any terminal emulator on Windows  (command prompt, PowerShell, etc) or any terminal on *nix systems. For the details, refer to the study guide and the Process Mining instruction.\n","###Question 17 (Trace frequency)\n","17. (a) Use the provided event log and identify the least frequent traces and the most frequent traces.\n"]},{"metadata":{"id":"tU_bbK2FBkK0","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_5_JFMrER2rf","colab_type":"text"},"cell_type":"markdown","source":["###Question 18 (Process Discovery and Conformance Checking using first filtered event log)\n","18. (a) Remove the two least frequent traces and create a new event log out of the original event log without the two least frequent traces."]},{"metadata":{"id":"UJmxAmghRodA","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_vjF6dm0RpQX","colab_type":"text"},"cell_type":"markdown","source":["18. (b) Use Inductive miner algorithm to discover the process model based on you new event log (the filtered log without two least frequent traces).\n"]},{"metadata":{"id":"NqlBAJYkQrWK","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bo5l2xWQXYQ_","colab_type":"text"},"cell_type":"markdown","source":["18. (c) Do the token replay conformance checking using your discovered model and the original event log. Does your process model fit?\n"]},{"metadata":{"id":"NDuHJDRiXxYQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"hf9VsGMEbKAN"},"cell_type":"markdown","source":["Your explanation:"]},{"metadata":{"id":"YzGYxMzGX0rL","colab_type":"text"},"cell_type":"markdown","source":["18. (ci) Calculate the fitness of your model."]},{"metadata":{"id":"-Yjjz-IyX6PR","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wmWIrp8GX_A_","colab_type":"text"},"cell_type":"markdown","source":["18. (cii) Are there any deviations between the process model and the event log?"]},{"metadata":{"colab_type":"text","id":"Ty8WFiocbFi3"},"cell_type":"markdown","source":["Your explanation:"]},{"metadata":{"id":"0PwxekClSCm4","colab_type":"text"},"cell_type":"markdown","source":["###Question 19 (Process Discovery and Conformance Checking using second filtered event log)\n","19. (a) Now use the original event log and remove the two most frequent traces, and discover the model based on your new event log(the filtered log without two most frequent traces).\n"]},{"metadata":{"id":"vPi8IokfSKPS","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_YKO3A-8SM7A","colab_type":"text"},"cell_type":"markdown","source":["19. (b) Do the token replay conformance checking using your newly discovered model and the original event log. Does your process model fit?"]},{"metadata":{"id":"IyWVfchYSRUh","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"u7y8iFRaa9uJ"},"cell_type":"markdown","source":["Your explanation:"]},{"metadata":{"id":"05uznBXGSVu5","colab_type":"text"},"cell_type":"markdown","source":["19. (bi) Calculate the fitness of your model?"]},{"metadata":{"id":"vmSpVfhoSan3","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"40v9719GYP8S","colab_type":"text"},"cell_type":"markdown","source":["19. (bii)  Is there any deviation inside the process model?"]},{"metadata":{"id":"secO-k-jayvG","colab_type":"text"},"cell_type":"markdown","source":["Your explanation:"]},{"metadata":{"id":"qXDGvC3RSpZu","colab_type":"text"},"cell_type":"markdown","source":["###Question 20 (Process Discovery using complete log)\n","20. (a) Use the complete event log (original event log) and discover your process model using inductive miner.\n"]},{"metadata":{"id":"eYrEUlz_SmE3","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XahMrZNvY0kn","colab_type":"text"},"cell_type":"markdown","source":["20. (b) Do the token replay conformance checking using your newly discovered model and the original event log. Does your process model fit?"]},{"metadata":{"id":"eN-EiMr6Y059","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b-g6ywFDatOY","colab_type":"text"},"cell_type":"markdown","source":["Your explanation:"]},{"metadata":{"id":"mqguSN3XY1Px","colab_type":"text"},"cell_type":"markdown","source":["20. (c) How are these three discovered process models different from each other? Which model is the best fitting to the original log? Why?"]},{"metadata":{"id":"oTdiAchtY_vg","colab_type":"text"},"cell_type":"markdown","source":["Your explanation:"]}]}