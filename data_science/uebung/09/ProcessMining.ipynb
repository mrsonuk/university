{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Mining\n",
    "## Inclusion\n",
    "In the lecture you've seen the basic concepts of Process Mining: the concepts of process, events, event data and process model; as well as what the unsupervised (Process Discovery) and supervised (Conformance Checking) parts of Process Mining are.\n",
    "\n",
    "You also installed pm4py, the Python Process Mining suite developed here in Aachen by the PADS team.\n",
    "\n",
    "**IMPORTANTE: THERE HAVE BEEN UPDATES AND BUGFIXES TO THE PACKAGE. PLEASE UPGRADE THE pm4py PACKAGE TYPING IN A TERMINAL:**\n",
    "\n",
    "`pip install pm4py --upgrade`\n",
    "\n",
    "To include it, and also to check that the installation has been successful, you can simply import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the command just returns the prompt and no error messages are dispayed it means that the inclusion - and thus the installation - was successful.\n",
    "\n",
    "Notice that, to allow for flexibility in the code and more importantly to allow for implementation of many versions of every algorithm, pm4py heavily relies on the **factory** design pattern: https://en.wikipedia.org/wiki/Factory_(object-oriented_programming).\n",
    "## Importing logs\n",
    "Let's start from the acquisition of an event log from a file. In this case, we are going to use the importer for the XES format.\n",
    "\n",
    "The XES format (eXtensible Event Stream) is a de-facto standard in Process Mining, and many of the most important event logs available in the community and in academic research are in this format. It follows the XML format.\n",
    "\n",
    "It is also possible to acquire the logs in the simpler CSV format, where you simply have each event represented by attributed in a line, separated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a XES event log\n",
    "from pm4py.objects.log.importer.xes import factory as xes_importer\n",
    "\n",
    "log = xes_importer.import_log('roadtraffic50traces.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `log` object is iterable, so you can treat it as a list of traces.\n",
    "A trace is simply modeled as a dictionary. The two keys in the trace dictionary are `attributes`, which contains a dictionary with the trace attributes, and `events`, which has a list as value. The list contains dictionaries: each entry in the list contains the event attributes, as pairs key-value.\n",
    "\n",
    "**Caveat**: in order to promote simplicity, the behaviour of the `trace` class is a list of events. Do not be fooled by the visualization! To access to the trace attributes, the  \n",
    "\n",
    "\n",
    "This way, you're able to manipulate logs, traces and events using the basic operations on list and dictionaries that we say in lecture 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log[0])\n",
    "print(len(log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now your turn!\n",
    "- Write a function to filter a log deleting all the traces shorter than a certain number of events (parameters: log, minimum length of traces).\n",
    "- Write a function to estract the set of resources from the event log.\n",
    "  - **Caveat**: not all the events have a resource in this log!\n",
    "- Write a function to filter a log deleting all the events not performed by a certain resource (parameters: log, resource to filter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other filtering functionalities\n",
    "In pm4py it is possible to process and filter an event log in many ways.\n",
    "Firstly, in case the imported log is not sorted, you can explicitly sort traces by timestamp of the first event with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you can filter on a certain time interval for traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering on timestamps\n",
    "from pm4py.algo.filtering.tracelog.timestamp import timestamp_filter\n",
    "\n",
    "filtered_log = timestamp_filter.filter_traces_contained(log, \"2007-03-09 00:00:00\", \"2012-01-18 23:59:59\")\n",
    "print(len(log))\n",
    "print(len(filtered_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This other version of timestamp filtering keeps also traces that intersect with the time interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.filtering.tracelog.timestamp import timestamp_filter\n",
    "\n",
    "filtered_log = timestamp_filter.filter_traces_intersecting(log, \"2007-03-09 00:00:00\", \"2012-01-18 23:59:59\")\n",
    "print(len(log))\n",
    "print(len(filtered_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful filter is the one on start and end activities. I can fetch the set of start and end activities of the log and filter on them in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing start and end activities and filtering on them\n",
    "from pm4py.algo.filtering.tracelog.start_activities import start_activities_filter\n",
    "\n",
    "log_start = start_activities_filter.get_start_activities(log)\n",
    "print(log_start)\n",
    "\n",
    "filtered_log = start_activities_filter.apply(log, ['Create Fine'])\n",
    "print(len(filtered_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.filtering.tracelog.end_activities import end_activities_filter\n",
    "\n",
    "log_end = end_activities_filter.get_end_activities(log)\n",
    "print(log_end)\n",
    "\n",
    "filtered_log = end_activities_filter.apply(log, ['Send for Credit Collection', 'Payment'])\n",
    "print(len(filtered_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mine for a model using the Inductive Miner\n",
    "Next, we are going to see how you discover automatically a process model from an event log. The discovery algorithm we are going to use is the one you have seen in detail in the lecture: the Inductive Miner.\n",
    "\n",
    "The Inductive Miner searches for a process tree that represents the process by building a directly-follow graph of the event log, and searching for a \"cut\": a separation of the graph that is then converted to a construct in a process tree. The procedure is repeated for the subgraph obtained by the cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining for a process tree\n",
    "from pm4py.algo.discovery.inductive import factory as inductive_miner\n",
    "\n",
    "tree = inductive_miner.apply_tree(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a process tree for the log we have acquired.\n",
    "\n",
    "Notice that, since a process tree is easily convertible in a Petri net, there is also the way to directly extract a Petri net (together with the respective markings for replay) with the Inductive Miner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining for a Petri net\n",
    "net, initial_marking, final_marking = inductive_miner.apply(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is especially useful when proceeding with other analysis on the model, since there are a huge variety of Process Mining algorithms applicable on Petri nets.\n",
    "## Visualizing models\n",
    "Once obtained the models I can also visualize them with Graphviz. Visualizing a model also works as a factory pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tree visualization\n",
    "from pm4py.visualization.process_tree import factory as pt_vis_factory\n",
    "\n",
    "gviz_tree = pt_vis_factory.apply(tree)\n",
    "pt_vis_factory.view(gviz_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Petri net visualization\n",
    "from pm4py.visualization.petrinet import factory as pn_vis_factory\n",
    "\n",
    "gviz_pn = pn_vis_factory.apply(net, initial_marking, final_marking)\n",
    "pn_vis_factory.view(gviz_pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly-follow graphs\n",
    "In the lecture, you have also seen another type of very simple process model: the directly-follow graph. You can create a directly follow graph with pm4py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining for a directly-follow graph\n",
    "from pm4py.algo.discovery.dfg import factory as dfg_factory\n",
    "\n",
    "dfg = dfg_factory.apply(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I can then visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly-follow graph visualization\n",
    "from pm4py.visualization.dfg import factory as dfg_vis_factory\n",
    "\n",
    "gviz_dfg = dfg_vis_factory.apply(dfg, log=log, variant='frequency')\n",
    "dfg_vis_factory.view(gviz_dfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default visualization decorated the graph with frequency information. To get the performance perspective, simply use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gviz_dfg = dfg_vis_factory.apply(dfg, log=log, variant='performance')\n",
    "dfg_vis_factory.view(gviz_dfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: why do we use the Inductive Miner (which builds a directly-follow graph as intermediate step) to mine for a process tree, instead of performing our analysis on the directly-follow graph?\n",
    "## Converting models\n",
    "Lastly, it is also possible to explicitly convert process models. As you have seen in the lecture, every process model is convertible into a Petri net (but not the opposite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting a process tree into a Petri net\n",
    "from pm4py.objects.conversion.tree_to_petri import factory as tree_petri_converter\n",
    "\n",
    "net, initial_marking, final_marking = tree_petri_converter.apply(tree)\n",
    "gviz_pn = pn_vis_factory.apply(net, initial_marking, final_marking)\n",
    "pn_vis_factory.view(gviz_pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Token-based Replay of an event log on a process model\n",
    "Next, having an event log and a process model it is possible to replay the log on top of the model obtaining replay metrics; more specifically, in the case of a Petri net it is possible to obtain token counts (produced, consumed, missing and remaining) for every single trace. The token-based replay implemented in pm4py allows to do it on an entire log with a one-liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing trace fitness\n",
    "from pm4py.algo.conformance.tokenreplay import factory as token_based_replay_factory\n",
    "print(log[0])\n",
    "token_replay_result = token_based_replay_factory.apply(log, net, initial_marking, final_marking)\n",
    "print(token_replay_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token-based replay returns an array of results (a result for each trace). As you can see, the results are a dictionary that  provides a number of pieces of information about the replay of that trace:\n",
    "- Whether if it fits the model or not\n",
    "- The trace fitness score\n",
    "- The sequence of activated transitions\n",
    "- The marking reached by the replay\n",
    "- The transitions enabled on the marking reached by the replay\n",
    "- The set of transitions where the deviations happened\n",
    "- The token counts:\n",
    "  - Missing tokens\n",
    "  - Consumed tokens\n",
    "  - Remaining tokens\n",
    "  - Produced tokens\n",
    "\n",
    "### Now your turn!\n",
    "Let's see what happens with a non conforming trace.\n",
    "The first trace is composed by just two event. Append a copy of the first event at the end opf the trace, and perform token replay. **Hint**: the token_based_replay_factory.apply method accepts a log (list of traces) as input, if you just pass a trace it won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the log fitness using the results of the Token-based Replay\n",
    "Getting the fitness on the whole event log requires an inclusion from the evaluation package of pm4py. What this method does is exactly the same, except for the result format that is returned: in this case we simply get the log fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing log fitness\n",
    "from pm4py.evaluation.replay_fitness import factory as replay_fitness_factory\n",
    "\n",
    "fitness_tokenbasedreplay = replay_fitness_factory.apply(log, net, initial_marking, final_marking)\n",
    "print(fitness_tokenbasedreplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
