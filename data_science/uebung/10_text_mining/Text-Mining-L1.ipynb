{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining\n",
    "In the first lecture of Text Mining you have seen how to preprocess the data, how to build a Bag of Words model, how to do tf-idf weighting and the model can be used in conjunction with other Data Science techniques.\n",
    "\n",
    "We are going to see how to put that in practice with Python. We are going to solve a document classification problem using the techniques seen in the lecture.\n",
    "\n",
    "The packages that we are going to use are `scikit-learn` and `nltk`. The first is the most complete Data Science library in Python, and contains quite a lot of utilities for Text Mining, while the second is more specialized and contains more advanced algorithms. In the second instruction we are going to see some advanced functionalities of `nltk`.\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "The first step is loading the corpus, which is `20newsgroups`, a collection of posts of different topics from newsgroups. It is contained directly in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training set part of 20newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can see the topics - the target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the categories (target attribute)\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fragment of document\n",
    "\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Bag of Words model\n",
    "The `CountVectorizer` method can be used to directly transform the dataset in a BoW model. This will first tokenize the text into words, and then create a vector space with one dimension for every word in the dictionary. Finally, it translates the documents in the corpora into count vectors of this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and construction of the Bag of Words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then build our first classifier using the count vector space. Notice that we are using the `pipeline` function of `scikit-learn`: we can specify a sequence of operations to be performed on the data. In this case, we apply the `CountVectorizer` and then an SVM classifier with linear kernel and stochastic gradient descent as solver for the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Mining pipeline v1: tokenization, BoW model, classification with SVM (linear kernel)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=3, random_state=42))])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf scoring\n",
    "Next, we can try to improve our results by introducing a tf-idf scoring step in the pipeline. We can use `TfidfTransformer` to convert the values of the vector from simple counts (tf) to tf-idf scores.\n",
    "\n",
    "The tf-idf calculation is done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the tf-idf score matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add it direcly to our `pipeline`, which now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Mining pipeline v2: tokenization, tf-idf scoring, BoW model, classification with SVM (linear kernel)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tf-idf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=3, random_state=42))])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal\n",
    "The next step would be to remove the stopwords. `CountVectorizer` has an integrated stoplist, and we can add an option to remove the stopwords as we are building the vector space. I can simply add a clause, and my `pipeline` now includes stopword removal and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Mining pipeline v3: tokenization, stopword removal, tf-idf scoring, BoW model, classification with SVM (linear kernel)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tf-idf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=3, random_state=42))])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming consists of \"chopping off\" a word eliminating the suffix and obtaining the root. We are not integrating stemming in the pipeline (it is quite heavy for the dataset and classifier we have here). Examples of stemmer can be found in the `nltk` package: we are going to see the `Snowball` stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "\n",
    "example = 'Process mining is a family of techniques in the field of process management that support the analysis of business processes based on event logs. During process mining, specialized data mining algorithms are applied to event log data in order to identify trends, patterns and details contained in event logs recorded by an information system. Process mining aims to improve process efficiency and understanding of processes.'\n",
    "wordlist = example.split(' ')\n",
    "print(example)\n",
    "print()\n",
    "print(' '.join([snowball_stemmer.stem(word) for word in wordlist]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the stemming procedure does not (always) (just) chop off the word. Modern stemmers also modify a little bit the root of the word in a further step of normalization (this cannot be considered lemmatization, because converging to the lemma is not the goal here).\n",
    "## Lemmatization\n",
    "You have also seen lemmatization: transforming a token in its lemma (base form). In `nltk` you can use the `WordNet` lemmatizer. Let's see the results of lemmatization as opposed to stemming, and compare them with the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "example = 'Process mining is a family of techniques in the field of process management that support the analysis of business processes based on event logs. During process mining, specialized data mining algorithms are applied to event log data in order to identify trends, patterns and details contained in event logs recorded by an information system. Process mining aims to improve process efficiency and understanding of processes.'\n",
    "wordlist = example.split(' ')\n",
    "print(example)\n",
    "print()\n",
    "print(' '.join([snowball_stemmer.stem(word) for word in wordlist]))\n",
    "print()\n",
    "print(' '.join([lemmatizer.lemmatize(word) for word in wordlist]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of lemmatization is way more complicated than stemming: and as you can see, as a result most lemmatizers are way more conservative than stemmers, in order to avoid introducting errors.\n",
    "\n",
    "The `WordNet` stemmer, however, accepts some indications of what the word really is. It is possible to pass a part-of-speech tag as parameter, and depending on how a word is interpreted, the lemma is different. The default behaviour of the `WordNet` lemmatizer is to consider everything a `NOUN`. Let's see how the result changes passing a `VERB` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('loving'))\n",
    "print(lemmatizer.lemmatize('loving', 'v'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
