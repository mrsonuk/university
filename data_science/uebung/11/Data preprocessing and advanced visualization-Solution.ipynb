{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous weeks we have already seen several data preprocessing methods. Recall, for example, the instruction session in week 2 on data exploration, basic visualization and decison trees or the instruction session on regression in week 3. We learned preprocessing steps such as\n",
    "\n",
    "* One-hot encoding\n",
    "* Outlier detection\n",
    "* Handling missing values\n",
    "* etc.\n",
    "\n",
    "Today we will show you a few more techniques. However, note that there are many possiblities to achieve your preprocessing goals in python and also this instruction just shows an extract of all possibilites.\n",
    "\n",
    "We will mostly use the *sklearn.preprocessing* package as well as *pandas* and *numpy*.\n",
    "\n",
    "### Missing values\n",
    "\n",
    "To illustrate the handling of missing values we will use a running example based on the diabetes data set. The data set has the following attributes:\n",
    "\n",
    "* O: Number of times pregnant\n",
    "* 1: Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "* 2: Diastolic blood pressure (mm Hg).\n",
    "* 3: Triceps skinfold thickness (mm).\n",
    "* 4: 2-Hour serum insulin (mu U/ml).\n",
    "* 5: Body mass index (weight in kg/(height in m)^2).\n",
    "* 6: Diabetes pedigree function.\n",
    "* 7: Age (years).\n",
    "* 8: Class variable (0 or 1).\n",
    "\n",
    "Let's first import the data set and use the *describe()* method to print a statistic summary on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary can already help us identify missing values. Recall from the lecture that not all missing values will be denoted as NAN. From the summary we see that several attributes have a minimum value of 0. From this, combined with domain knowledge we can conclude that for several attributes the 0 value represents a missing value.\n",
    "These attributes are:\n",
    "\n",
    "* 1: Plasma glucose concentration\n",
    "* 2: Diastolic blood pressure\n",
    "* 3: Triceps skinfold thickness\n",
    "* 4: 2-Hour serum insulin\n",
    "* 5: Body mass index\n",
    "\n",
    "We are interested in how many of these missing values we have per attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((dataset[[1,2,3,4,5]] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different numbers in missing values illustrates the need for the different strategies on handling missing data. \n",
    "\n",
    "First of all we are going to transform our data, such that the invalid 0 values are actually represented as NaN. This is convenient because NaN values are ignored by functions such as sum or count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "#replace 0 values by NaN values\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, numpy.NaN)\n",
    "#count the number of NaN values to see whether we replaced all required values\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove data entries with missing values\n",
    "The simplest strategie to handle missing values is simply deleting all records that contain missing values. Pandas provides a simple function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows and columns of the original dataset\n",
    "print(dataset.shape)\n",
    "# drop rows with missing values\n",
    "dataset.dropna(inplace=True)\n",
    "# count the number of rows and columns left in the dataset\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing values\n",
    "Simply removing all data entries that contain missing values reduces the data set significantly. This might alter the results we obtain from models trained on this data set (e.g. regression models, decision trees etc.). Therefore we need other methods:\n",
    "\n",
    "* Filling in a constant value obtained through domain knowledge\n",
    "* A value from another randomly selected record\n",
    "* Mean/ median/ mode value of the attribute\n",
    "* A value estimated by another predictive model\n",
    "\n",
    "We can use the function *fillna()* from the pandas package to fill our missing values. The first argument of the function specifies the value that should replace the missing values. In the example below we use the mean of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, numpy.NaN)\n",
    "# fill missing values with mean column values\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "# count the number of NaN values in each column\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can also use the *Imputer* provided by the *sklearn.impute* package. The example below demonstrates its usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, numpy.NaN)\n",
    "\n",
    "imputer = SimpleImputer(missing_values=numpy.nan, strategy = \"mean\")\n",
    "transformed_values = imputer.fit_transform(dataset)\n",
    "\n",
    "print(numpy.isnan(transformed_values).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "Handling missing values should have an effect on the quality of our trained models. To show this take the *class-grades dataset* and train two different linear regression models to predict the final grade. Compare their accuracy scores.\n",
    "\n",
    "* Model trained on a data set with the missing values deleted.\n",
    "* Model trained on a data set with the missing values replaced by the mean of the attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "grades = read_csv('class-grades.csv',sep=\",\", error_bad_lines=False)\n",
    "#delete rows with missing values\n",
    "grades_del= grades.dropna()\n",
    "X1 = grades_del.iloc[:,0:5]\n",
    "y1 = numpy.reshape([grades_del.iloc[:,-1]],(-1, 1))\n",
    "\n",
    "#replace missing values with mean\n",
    "grades_mean = grades.fillna(grades.mean(), inplace=False)\n",
    "X2 = grades_mean.iloc[:,0:5]\n",
    "y2 = numpy.reshape([grades_mean.iloc[:,-1]],(-1, 1))\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#define the classifier\n",
    "classifier = LinearRegression()\n",
    "\n",
    "#train the classifier with first dataset\n",
    "classifier.fit(X1, y1)\n",
    "y1_pred = classifier.predict(X1)\n",
    "#train the classifier with second dataset\n",
    "classifier.fit(X2, y2)\n",
    "y2_pred = classifier.predict(X2)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y1, y1_pred))\n",
    "print(mean_squared_error(y2, y2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling / Normalization / Standardization\n",
    "\n",
    "The terms scaling, normalization and standardization are often used interchangable. Most often normalization aims to rescale values into a certain range (e.g. [0,1]) and standardization rescales data to have a mean of 0 and a standard deviation of 1. **Note, that the sklearn.preprocessing package has chosen the general term \"scaling\" for both, normalization and standardization operations.**\n",
    "\n",
    "In general, aside from the name of a function, it is important to know how it transforms your data and whether that is what you are aiming to achieve.\n",
    "\n",
    "#### Scaling attributes to a range\n",
    "The *MinMaxScaler* transforms each feature to a given range (default: [0,1]).\n",
    "\n",
    "The transformation is given by:\n",
    "\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "data = numpy.array([[ 1., -1.,  2.], [ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    "\n",
    "#creating the scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler() \n",
    "#fitting the scaler to the data\n",
    "min_max_scaler.fit(data)\n",
    "#printing the min and max values of the given data\n",
    "print(min_max_scaler.data_min_)\n",
    "print(min_max_scaler.data_max_)\n",
    "#printing the range of the given data\n",
    "print(min_max_scaler.data_range_)\n",
    "\n",
    "#transforming the data to the defined new range\n",
    "transformed_data = min_max_scaler.transform(data)\n",
    "print(transformed_data)\n",
    "\n",
    "#We can also transform data other than the ones used to fit the scaler\n",
    "print(min_max_scaler.transform([[2,2,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *MaxAbsScaler* scales each attribute such that the maximum absolute value of each feature in the training set will be 1.0. It's functions are similar to the MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.array([[ 1., -1.,  2.], [ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    "\n",
    "#creating the scaler\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler() \n",
    "max_abs_scaler.fit(data)\n",
    "\n",
    "#transforming the data to the defined new range\n",
    "transformed_data = max_abs_scaler.transform(data)\n",
    "print(transformed_data)\n",
    "\n",
    "#We can also transform data other than the ones used to fit the scaler\n",
    "print(max_abs_scaler.transform([[2,2,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling to have a mean of 0 and standard deviation of 1\n",
    "\n",
    "For many algorithms problems arise if a feature has a variance that is orders of magnitude larger than others. It might dominate the objective function and make the estimator unable to learn from other features correctly as expected. Therefore we transform our data which can be easily done with the *scale* function provided by the *sklearn.preprocessing* package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.array([[ 1., -1.,  2.], [ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    "scaled_data = preprocessing.scale(data)\n",
    "print(scaled_data)\n",
    "print(scaled_data.mean(axis=0))\n",
    "print(scaled_data.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "Scaling data can have an effect on the quality of our trained models. To show this take the *wine dataset* and train three different logistic regression models predicting the target. Compare their accuracy scores.\n",
    "\n",
    "* Model trained on the original data set\n",
    "* Model trained on a data set scaled using the MinMaxScaler.\n",
    "* Model trained on a data set scaled to the attributes having a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn import datasets\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "X1 = wine['data']\n",
    "y1 = wine['target']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#define the classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "#train the classifier with first dataset\n",
    "classifier.fit(X1, y1)\n",
    "print(classifier.score(X1,y1))\n",
    "\n",
    "#MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler() \n",
    "#fitting the scaler to the data\n",
    "min_max_scaler.fit(X1)\n",
    "#transforming the data to the defined new range\n",
    "X2 = min_max_scaler.transform(X1)\n",
    "\n",
    "classifier.fit(X2, y1)\n",
    "print(classifier.score(X2,y1))\n",
    "\n",
    "#Scaling to mean of 0 and std of 1\n",
    "X3 = preprocessing.scale(X1)\n",
    "classifier.fit(X3, y1)\n",
    "print(classifier.score(X3,y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization/ Binning\n",
    "Discretization provides a way to partition continuous features into discrete values. \n",
    "\n",
    "The KBinsDiscretizer discretizes attributes into k equal width bins. \n",
    "\n",
    "By default the output is one-hot encoded and this can be configured with the *encode* parameter:\n",
    "\n",
    "* Onehot: Encodes the results with one-hot encoding and returns a sparse matrix.\n",
    "* Onehot-dense: Encodes the results with one-hot encoding and returns a dense array.\n",
    "* Ordinal: return the bin identifier encoded as an integer value.\n",
    "\n",
    "By default the data is split into bins with equal number of data points. This can be configured with the *strategy* parameter:\n",
    "\n",
    "* Uniform: All bins in each feature have identical width\n",
    "* Quantile: All bins in each feature have the same number of points.\n",
    "* kmeans: Values in each bin have the same nearest center of 1D k-means cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.array([[ -3., 5., 15 ],[  0., 6., 14 ],[  6., 3., 11 ]])\n",
    "discretizer = preprocessing.KBinsDiscretizer(n_bins=[3,2,2], encode='ordinal', strategy = 'uniform')\n",
    "discretizer.fit(data)\n",
    "discretized_data = discretizer.transform(data)\n",
    "#showing the transformed data\n",
    "print(discretized_data)\n",
    "\n",
    "#displaying the edges of each bin per attribute\n",
    "print(discretizer.bin_edges_[0])\n",
    "print(discretizer.bin_edges_[1])\n",
    "print(discretizer.bin_edges_[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced visualizations\n",
    "Recall basic visualization techniques from instruction session 2, such as box plots, bar charts etc. Today we will work with more advanced techniques.\n",
    "\n",
    "The following section shows an extract of the various visualization possibilities. Python provides the advantage that the plots can be created with a few simple lines of code.\n",
    "\n",
    "## Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# we use the iris dataset\n",
    "import seaborn as sns\n",
    "data = sns.load_dataset('iris')\n",
    "\n",
    "# Make the plot\n",
    "scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Coordinate plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Make the plot\n",
    "parallel_coordinates(data, 'species', colormap=plt.get_cmap(\"Set2\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamgraph\n",
    "To visualize a streamgraph in Python we use make use of stackplots from the *matplotlib* package. Stackplots are generated by plotting different datasets vertically on top of one another rather than overlapping with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the values for our x-axis\n",
    "x = [1, 2, 3, 4, 5]\n",
    "# the values that will be stacked on top of each other\n",
    "y1 = [1, 1, 2, 3, 5]\n",
    "y2 = [0, 4, 2, 6, 8]\n",
    "y3 = [1, 3, 5, 7, 9]\n",
    "\n",
    "# the labels for y1, y2 and y3\n",
    "labels = [\"Fibonacci \", \"Evens\", \"Odds\"]\n",
    "\n",
    "#stacking our values vertically\n",
    "y = numpy.vstack([y1, y2, y3])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#modifying the axis\n",
    "ax.stackplot(x, y1, y2, y3, labels=labels, baseline='wiggle')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap\n",
    "We use the heatmap function from the *seaborn package* to create a heatmap. Note that we have to aggregate/pivot our data into the correct shape before the heatmap can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example flights dataset and conver to long-form\n",
    "flights_long = sns.load_dataset(\"flights\")\n",
    "flights = flights_long.pivot(\"month\", \"year\", \"passengers\")\n",
    "\n",
    "# Draw a heatmap with the numeric values in each cell\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.heatmap(flights, annot=True, fmt=\"d\", linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "In real-life no one is going to tell you which visualization technique to use for which data! \n",
    "Pretend that you are a data scientist who was just provided with the datasets used in this instruction session. Visualize the data using the previously presented techniques and/or other suitable techniques provided by the matplotlib (https://matplotlib.org/gallery/index.html) and the seaborn packages (https://seaborn.pydata.org/examples/index.html). Decide by yourself which part of the data you want to visualize using which plot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
