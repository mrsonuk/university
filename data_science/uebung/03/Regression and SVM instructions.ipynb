{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in previous lectures we need to import several packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a data set with a independent variable/feature x and a response value y. We try to find a linear function that predicts y as accurately as possible as a function of the independent variable x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'col2': [1, 3, 2, 5, 7, 8, 8, 9, 10, 12]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the previous instruction session. Let's have a look at the data in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df.col1, df.col2, color = \"m\", marker = \"o\", s = 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: we want to fit a linear function that describes the data in the best way possible. We use linear regression from the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the data such that the function can handle it\n",
    "X = np.reshape([df.col1],(-1, 1))\n",
    "y = np.reshape([df.col2],(-1, 1))\n",
    "\n",
    "#define the classifier\n",
    "classifier = LinearRegression()\n",
    "#train the classifier\n",
    "model = classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use that classifier to predict y. We print the predictions as well as the coefficient and intercept of the linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the classifier to make prediction\n",
    "y_predict = np.array(classifier.predict(X))\n",
    "print(y_predict)\n",
    "#print coefficient and intercept\n",
    "print('Coefficients: \\n', classifier.coef_)\n",
    "print('Intercept: \\n', classifier.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our regression function. Herefore we use the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize data points\n",
    "plt.scatter(df.col1, df.col2, color = \"m\", marker = \"o\", s = 30) \n",
    "#visualize regression function\n",
    "plt.plot(X, y_predict, color = \"g\") \n",
    "plt.xlabel('x') \n",
    "plt.ylabel('y') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span> Build a simple linear regression for the data below. Use col1 as independent variable and col2 as target feature. Also plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'col1': [770, 677, 428, 410, 371, 504, 1136, 695, 551, 550], 'col2': [54, 47, 28, 38, 29, 38, 80, 52, 45, 40]})\n",
    "#your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the model and its predictions is often not enough. Let's also compute their error. The sklearn.metrics package contains several errors such as\n",
    "\n",
    "* Mean squared error\n",
    "* Mean absolute error\n",
    "* Mean squared log error\n",
    "* Median absolute error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize our squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_predict, (y - y_predict) ** 2, color = \"blue\", s = 10,) \n",
    "  \n",
    "## plotting line for zero error \n",
    "plt.hlines(y = 0, xmin = 0, xmax = 15, linewidth = 2) \n",
    "  \n",
    "## plot title \n",
    "plt.title(\"Squared errors\") \n",
    "  \n",
    "## function to show plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span> Compute the mean squared error and visualize the squared errors. Play around using different error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling multiple independent variables at once\n",
    "In most cases, we will have more than one independent variableâ€Š. There can be as little as two independent variables and up to hundreds of variables. As an example we use an example data set of the scikit package. The dataset describes housing prices in Boston based on several attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets ## imports datasets from scikit-learn\n",
    "data = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the housing price we will use a Multiple Linear Regression model. In Python this is very straightforward: we use the same function as for simple linear regression but our variable X consists of multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "classifier2 = LinearRegression()\n",
    "model2 = classifier2.fit(X,y)\n",
    "\n",
    "y_predict2 = classifier2.predict(X)\n",
    "print(y_predict2[0:5])\n",
    "\n",
    "print('Coefficients: \\n', classifier2.coef_)\n",
    "print('Intercept: \\n', classifier2.intercept_)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y, y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling categorical independent variables \n",
    "So far we always encountered numerical independent variables, but data sets can also contain categorical attributes. The regression function can only handle numerical input. Therefore we need to tranform our data using one-hot encoding as explained in the lecture: we introduce a 0/1 feature for every possible value of our categorical attribute.\n",
    "\n",
    "There are two popular possibilities to achieve this\n",
    "\n",
    "* the get_dummies function of pandas\n",
    "* the OneHotEncoder of scikit\n",
    "\n",
    "After encoding the attributes we can apply our regular regression function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example using pandas\n",
    "\n",
    "df = pd.DataFrame({'A':['a','b','c'],'B':['c','b','a'] })\n",
    "one_hot_pd = pd.get_dummies(df)\n",
    "one_hot_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example using scikit\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "# encode labels with value between 0 and n_classes-1.\n",
    "df2 = df.apply(le.fit_transform)\n",
    "df2.head()\n",
    "\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "encoder.fit(df2)\n",
    "\n",
    "onehotlabels = encoder.transform(df2).toarray()\n",
    "onehotlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span>  Perform linear regression using the data set given below. Don't forget to transform your categorical independent variables. The rental price attribute represents the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your turn\n",
    "data = pd.DataFrame({'Size':[500,550,620,630,665],'Floor':[4,7,9,5,8], 'Energy rating':['C', 'A', 'A', 'B', 'C'], 'Rental price': [320,380,400,390,385] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a categorical target value - Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also encounter data sets where our target value is categorical. Here we don't transform them into numeric values, but we use a logistic regression function. Luckily, sklearn provides us with a suitable function that is similar to the linear equivalent. Similar to linear regression we can compute logistic regression on a single independent variable as well as multiple.\n",
    "Regular logistic regression handles target variables with binary values, but the algorithm is also able"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "iris = pd.read_csv(r\"C:\\Users\\syring\\Documents\\iris.csv\", index_col = None)\n",
    "X = iris.iloc[:,0:1] #we only use the first attribute\n",
    "y = iris.iloc[:,-1]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier3 = LogisticRegression( solver = 'liblinear', multi_class = 'ovr')\n",
    "classifier3.fit(X, y.ravel())\n",
    "\n",
    "y_pred = classifier3.predict(X)\n",
    "print(y_pred[0:5])\n",
    "\n",
    "print('Coefficients: \\n', classifier3.coef_)\n",
    "print('Intercept: \\n', classifier3.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span>  In the example above we only used the first attribute as independent variable. Change the example to be able to handle all available attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from regression models, the sklearn package also provides us with a function for training support vector machines. Looking at the example below we see that they can be trained in similar ways. We still use the iris data set for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X = iris.iloc[:,0:3] \n",
    "y = iris.iloc[:,-1]\n",
    "\n",
    "classifier3 = SVC(C=1, kernel='linear', gamma = 'auto')\n",
    "classifier3.fit(X, y)\n",
    "\n",
    "y_pred2 = classifier3.predict(X)\n",
    "y_pred2[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the lecture, a support vector machine is defined by its support vectors. In the sklearn package we can access them and their properties very easily.\n",
    "\n",
    "* support_: indicies of support vectors\n",
    "* support_vectors_: the support vectors\n",
    "* n_support_: the number of support vectors for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier3.support_)\n",
    "print(classifier3.support_vectors_)\n",
    "print(classifier3.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the distance of the samples X to the separating hyperplane by using the decision_function(X) method. Score(X,y) calculates the mean accuracy of the classification. The classification report shows measures such as precision, recall, f1-score and support. You will learn more about these quality measures in a few lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classifier3.decision_function(X)\n",
    "print('Accuracy: \\n', classifier3.score(X,y_pred2))\n",
    "print(classification_report(y, y_pred2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC has many parameters. In the lecture you learned about the concept of kernels. Scikit gives you the opportunity to try different kernel functions.\n",
    "Furthermore, the parameter C tells the SVM optimization how much you want to avoid misclassifying each training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span>  Play around with the parameter settings of the SVM using the previous iris example. Which settings lead to good classifications? \n",
    "On the scikit website you can find more information about the available kernels etc. http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
