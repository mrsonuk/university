{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining: doc2vec\n",
    "In the second Text Mining lecture you have learned about some more advanced model and techniques to analyze text: n-grams and word2vec/doc2vec. In this instruction we are going to see an example of how you can train a do2vec model and use it for text classification.\n",
    "\n",
    "For this we are going to use the 20newsgroups corpus again, where the documents are newsgroups posts and the label is the newsgroup the post was published in (and thus the topic).\n",
    "\n",
    "Let's first fetch the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training set part of 20newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, as shown below, targets are not strings but numbers. The target_names attribute allows us to fetch the list of labels: targets are indexes in this list of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.autos'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Let's preprocess the text. For this test, we are not going to normalize the text, but we will only tokenize it. The gensim tool `gensim.utils.simple_preprocess` tokenizes a text, puts everything in lowercase and eliminates punctuation.\n",
    "\n",
    "Gensim's doc2vec needs a list of TaggedDocument objects in input. A TaggedDocument is creating with two explicit parameters: `words`, which has to be a list of strings (tokens) and `tags`, which has to be a list of strings (labels). In our case, the label is unique, so we have to use a list with just one element (targets are lists because TaggedDocument also supports multilabel classification). Using the syntax above we fetch the string label for each document and we create TaggedDocuments.\n",
    "We do this for both training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pegoraro\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(words=['from', 'lerxst', 'wam', 'umd', 'edu', 'where', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst'], tags=['rec.autos'])\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing, normalizing, and creating lists of TaggedDocument objects\n",
    "import gensim\n",
    "\n",
    "twenty_train_tagged = []\n",
    "twenty_test_tagged = []\n",
    "\n",
    "for i in range (0, len(twenty_train.data)):\n",
    "    twenty_train_tagged.append(gensim.models.doc2vec.TaggedDocument(words=gensim.utils.simple_preprocess(twenty_train.data[i]), tags=[twenty_train.target_names[twenty_train.target[i]]]))\n",
    "\n",
    "for i in range (0, len(twenty_test.data)):\n",
    "    twenty_test_tagged.append(gensim.models.doc2vec.TaggedDocument(words=gensim.utils.simple_preprocess(twenty_test.data[i]), tags=[twenty_test.target_names[twenty_test.target[i]]]))\n",
    "\n",
    "print(repr(twenty_train_tagged[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up a bit the calculations, let's fetch the number of cores the machine has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a vocabulary\n",
    "\n",
    "At this point, we are ready to train our doc2vec model. The first thing to do is to create the vocabulary, in order to determine the sizes of input and output and also build the one-hot encoding for tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 2310001.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary\n",
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "doc2vec_model = Doc2Vec(dm=0, vector_size=40, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "doc2vec_model.build_vocab([x for x in tqdm(twenty_train_tagged)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a document embedding\n",
    "Once created the object for the model and the vocabulary, it is time to train the encoding neural network that will provide the representation. The hyperparameters are the regular ones for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1274079.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 945400.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1137394.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 2561638.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1612229.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1821524.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1890913.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 2857835.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1498258.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1448678.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1410359.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1898705.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1877521.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 945437.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 2774135.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1620930.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 2198895.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1410317.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1427284.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1620653.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 941386.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1891365.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1141003.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1620874.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1890687.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1441549.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 2269457.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1418071.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1890536.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11314/11314 [00:00<00:00, 1129892.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training the doc2vec model\n",
    "from sklearn import utils\n",
    "\n",
    "for epoch in range(30):\n",
    "    doc2vec_model.train(utils.shuffle([x for x in tqdm(twenty_train_tagged)]), total_examples=len(twenty_train_tagged), epochs=1)\n",
    "    doc2vec_model.alpha -= 0.002\n",
    "    doc2vec_model.min_alpha = doc2vec_model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the document features vector space\n",
    "Once trained the doc2vec representation model, we can use it to convert documents to fixed-length vectors in order to use these vectors in a classifier. The method `infer_vector` can be used for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the feature vector for the classifier\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    targets, doc2vec_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in tagged_docs])\n",
    "    return targets, doc2vec_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating docs into vectors for training and test set\n",
    "y_train, X_train = vec_for_learning(doc2vec_model, twenty_train_tagged)\n",
    "y_test, X_test = vec_for_learning(doc2vec_model, twenty_test_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training a classifier\n",
    "Finally, we can create a classifier with the usual syntax, and evaluate the results using the usual performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a classification model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification performance metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
